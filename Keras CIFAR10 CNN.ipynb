{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanawit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/thanawit/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Activation, Flatten, Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import os \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialte the variable\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation  = True\n",
    "num_predictions = 20 \n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the dataset\n",
    "\n",
    "(x_train , y_train) , (x_test , y_test ) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert class to binary class\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test =  keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Layer\n",
    "\n",
    "model = Sequential()\n",
    "#Conv2D Filter = filter size, kernel size, stride , padding , format , dilation rate , activation , biased, initializer, bias initilizaer, more \n",
    "\n",
    "#Block 1 \n",
    "model.add(Conv2D( 32 , (3,3) , padding='same',input_shape = x_train.shape[1:]  ))\n",
    "# 2. Using ReLU instead of Sigmoid\n",
    "model.add(Activation('relu'))\n",
    "# 5. Using BatchNorm to Increase Accuracy\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32 , (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# 3. Using Dropout to Increase Accura\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Block 2 \n",
    "model.add(Conv2D(64, (3,3) , padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64 , (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Connecting it to Classification\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Scheduling LR decaying \n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "#initiate training lost\n",
    "#4. Second parameter is decayed \n",
    "opt = keras.optimizers.rmsprop(lr_schedule(0), decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#meaning all the result\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract mean to make it more normalize\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_test -= x_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 13s - loss: 1.5007 - acc: 0.4933 - val_loss: 1.1968 - val_acc: 0.5985\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 12s - loss: 1.0183 - acc: 0.6464 - val_loss: 0.8722 - val_acc: 0.6991\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.8539 - acc: 0.7052 - val_loss: 0.9177 - val_acc: 0.6866\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.7728 - acc: 0.7361 - val_loss: 0.7685 - val_acc: 0.7404\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.7114 - acc: 0.7561 - val_loss: 0.7498 - val_acc: 0.7455\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.6584 - acc: 0.7758 - val_loss: 0.6946 - val_acc: 0.7695\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.6209 - acc: 0.7879 - val_loss: 0.7751 - val_acc: 0.7356\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.5853 - acc: 0.8030 - val_loss: 0.6937 - val_acc: 0.7713\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.5555 - acc: 0.8134 - val_loss: 0.6837 - val_acc: 0.7772\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 13s - loss: 0.5202 - acc: 0.8230 - val_loss: 0.6790 - val_acc: 0.7863\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.4991 - acc: 0.8322 - val_loss: 0.7466 - val_acc: 0.7547\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.4682 - acc: 0.8410 - val_loss: 0.6432 - val_acc: 0.7909\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.4516 - acc: 0.8462 - val_loss: 0.6182 - val_acc: 0.8013\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.4415 - acc: 0.8509 - val_loss: 0.5869 - val_acc: 0.8047\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.4132 - acc: 0.8602 - val_loss: 0.6964 - val_acc: 0.7729\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.4015 - acc: 0.8635 - val_loss: 0.5937 - val_acc: 0.8065\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3813 - acc: 0.8708 - val_loss: 0.6968 - val_acc: 0.7906\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3687 - acc: 0.8745 - val_loss: 0.6158 - val_acc: 0.8173\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3602 - acc: 0.8773 - val_loss: 0.6395 - val_acc: 0.8006\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3478 - acc: 0.8818 - val_loss: 0.6145 - val_acc: 0.8200\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3286 - acc: 0.8869 - val_loss: 0.6367 - val_acc: 0.7947\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3253 - acc: 0.8901 - val_loss: 0.5883 - val_acc: 0.8207\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3117 - acc: 0.8954 - val_loss: 0.7311 - val_acc: 0.7995\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.3014 - acc: 0.8992 - val_loss: 0.6311 - val_acc: 0.7989\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2954 - acc: 0.8986 - val_loss: 0.7923 - val_acc: 0.7942\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2805 - acc: 0.9038 - val_loss: 0.7065 - val_acc: 0.7774\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2778 - acc: 0.9065 - val_loss: 0.6255 - val_acc: 0.8121\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2718 - acc: 0.9087 - val_loss: 0.7202 - val_acc: 0.8008\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2667 - acc: 0.9091 - val_loss: 0.5652 - val_acc: 0.8205\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2542 - acc: 0.9122 - val_loss: 0.7660 - val_acc: 0.7959\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2465 - acc: 0.9164 - val_loss: 0.6008 - val_acc: 0.8302\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2434 - acc: 0.9163 - val_loss: 0.7152 - val_acc: 0.7791\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2426 - acc: 0.9191 - val_loss: 0.6032 - val_acc: 0.8237\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2353 - acc: 0.9209 - val_loss: 0.6900 - val_acc: 0.8154\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2319 - acc: 0.9218 - val_loss: 0.7457 - val_acc: 0.8124\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2251 - acc: 0.9235 - val_loss: 0.6909 - val_acc: 0.8183\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2206 - acc: 0.9257 - val_loss: 0.6357 - val_acc: 0.8197\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2130 - acc: 0.9279 - val_loss: 0.6211 - val_acc: 0.8311\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2082 - acc: 0.9296 - val_loss: 0.6083 - val_acc: 0.8222\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2043 - acc: 0.9310 - val_loss: 0.7214 - val_acc: 0.8157\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2071 - acc: 0.9303 - val_loss: 0.5983 - val_acc: 0.8257\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.2033 - acc: 0.9313 - val_loss: 0.5959 - val_acc: 0.8386\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1998 - acc: 0.9324 - val_loss: 0.7334 - val_acc: 0.8051\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1922 - acc: 0.9355 - val_loss: 0.7038 - val_acc: 0.8083\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1914 - acc: 0.9350 - val_loss: 0.7806 - val_acc: 0.8221\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1931 - acc: 0.9357 - val_loss: 0.6254 - val_acc: 0.8197\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1879 - acc: 0.9378 - val_loss: 0.7213 - val_acc: 0.8138\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1830 - acc: 0.9378 - val_loss: 0.8007 - val_acc: 0.7982\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1765 - acc: 0.9406 - val_loss: 0.7692 - val_acc: 0.8087\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1768 - acc: 0.9407 - val_loss: 0.7018 - val_acc: 0.8233\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1758 - acc: 0.9404 - val_loss: 0.7301 - val_acc: 0.7982\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1706 - acc: 0.9425 - val_loss: 0.6663 - val_acc: 0.8245\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1730 - acc: 0.9415 - val_loss: 0.6209 - val_acc: 0.8283\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1704 - acc: 0.9441 - val_loss: 0.6127 - val_acc: 0.8336\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1678 - acc: 0.9441 - val_loss: 0.6659 - val_acc: 0.8290\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1691 - acc: 0.9426 - val_loss: 0.6381 - val_acc: 0.8182\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1680 - acc: 0.9449 - val_loss: 0.6114 - val_acc: 0.8216\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1624 - acc: 0.9463 - val_loss: 0.6994 - val_acc: 0.8193\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1639 - acc: 0.9464 - val_loss: 0.6817 - val_acc: 0.8298\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1650 - acc: 0.9465 - val_loss: 0.6150 - val_acc: 0.8242\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1623 - acc: 0.9472 - val_loss: 0.6382 - val_acc: 0.8291\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1562 - acc: 0.9484 - val_loss: 0.6722 - val_acc: 0.8262\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1582 - acc: 0.9482 - val_loss: 0.7157 - val_acc: 0.8244\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1545 - acc: 0.9499 - val_loss: 0.6339 - val_acc: 0.8131\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1572 - acc: 0.9483 - val_loss: 0.6981 - val_acc: 0.8099\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1536 - acc: 0.9491 - val_loss: 0.7487 - val_acc: 0.8156\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1554 - acc: 0.9485 - val_loss: 0.6471 - val_acc: 0.8160\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1525 - acc: 0.9498 - val_loss: 0.7167 - val_acc: 0.8169\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1516 - acc: 0.9496 - val_loss: 0.7691 - val_acc: 0.8078\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1550 - acc: 0.9497 - val_loss: 0.7391 - val_acc: 0.7998\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1537 - acc: 0.9504 - val_loss: 0.7485 - val_acc: 0.8108\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1551 - acc: 0.9501 - val_loss: 0.7097 - val_acc: 0.8240\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1552 - acc: 0.9502 - val_loss: 0.6753 - val_acc: 0.8170\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1501 - acc: 0.9504 - val_loss: 0.7558 - val_acc: 0.8113\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1500 - acc: 0.9522 - val_loss: 0.7622 - val_acc: 0.8290\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1468 - acc: 0.9522 - val_loss: 0.7017 - val_acc: 0.8104\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1491 - acc: 0.9522 - val_loss: 0.6885 - val_acc: 0.8175\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1452 - acc: 0.9521 - val_loss: 0.6852 - val_acc: 0.8300\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1441 - acc: 0.9527 - val_loss: 0.8501 - val_acc: 0.8063\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1424 - acc: 0.9530 - val_loss: 0.7412 - val_acc: 0.8216\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1496 - acc: 0.9512 - val_loss: 0.7454 - val_acc: 0.7995\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1504 - acc: 0.9519 - val_loss: 0.6933 - val_acc: 0.8238\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1469 - acc: 0.9530 - val_loss: 0.7026 - val_acc: 0.8191\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1457 - acc: 0.9527 - val_loss: 0.7602 - val_acc: 0.8078\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1454 - acc: 0.9522 - val_loss: 0.8055 - val_acc: 0.8279\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1460 - acc: 0.9541 - val_loss: 0.8521 - val_acc: 0.7971\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1497 - acc: 0.9507 - val_loss: 0.9391 - val_acc: 0.7982\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1442 - acc: 0.9531 - val_loss: 0.6777 - val_acc: 0.8251\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1420 - acc: 0.9554 - val_loss: 0.8659 - val_acc: 0.8163\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1427 - acc: 0.9527 - val_loss: 0.7462 - val_acc: 0.8218\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1430 - acc: 0.9545 - val_loss: 0.6472 - val_acc: 0.8155\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1398 - acc: 0.9550 - val_loss: 0.7585 - val_acc: 0.8246\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1467 - acc: 0.9525 - val_loss: 0.7972 - val_acc: 0.8172\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1448 - acc: 0.9536 - val_loss: 0.7157 - val_acc: 0.8286\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1386 - acc: 0.9566 - val_loss: 0.6661 - val_acc: 0.8252\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1428 - acc: 0.9540 - val_loss: 0.7328 - val_acc: 0.8245\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1454 - acc: 0.9542 - val_loss: 0.7024 - val_acc: 0.8235\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1449 - acc: 0.9537 - val_loss: 0.7159 - val_acc: 0.8123\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1412 - acc: 0.9541 - val_loss: 0.7977 - val_acc: 0.8263\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 12s - loss: 0.1352 - acc: 0.9575 - val_loss: 0.7987 - val_acc: 0.8155\n",
      "Saved trained model at /home/thanawit/Softcom Final Project/saved_models/keras_cifar10_trained_model.h5 \n"
     ]
    }
   ],
   "source": [
    "#Decaying LR\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [ lr_reducer, lr_scheduler]\n",
    "\n",
    "\n",
    "#train model \n",
    "# model.fit(x_train , y_train, batch_size=batch_size , epochs= epochs , validation_data=(x_test, y_test), shuffle=True , callbacks=callbacks)\n",
    "\n",
    "model.fit(x_train , y_train, batch_size=batch_size , epochs= epochs , validation_data=(x_test, y_test), shuffle=True )\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9760/10000 [============================>.] - ETA: 0sTest loss: 0.798695784664154\n",
      "Test accuracy: 0.8155\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
