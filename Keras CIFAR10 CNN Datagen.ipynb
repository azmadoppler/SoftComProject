{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanawit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/thanawit/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, Activation, Flatten, Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import os \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialte the variable\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation  = True\n",
    "num_predictions = 20 \n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the dataset\n",
    "\n",
    "(x_train , y_train) , (x_test , y_test ) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert class to binary class\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test =  keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Layer\n",
    "\n",
    "model = Sequential()\n",
    "#Conv2D Filter = filter size, kernel size, stride , padding , format , dilation rate , activation , biased, initializer, bias initilizaer, more \n",
    "\n",
    "#Block 1 \n",
    "model.add(Conv2D( 32 , (3,3) , padding='same',input_shape = x_train.shape[1:]  ))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32 , (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Block 2 \n",
    "model.add(Conv2D(64, (3,3) , padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64 , (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#Connecting it to Classification\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "#initiate training lost \n",
    "opt = keras.optimizers.rmsprop(lr_schedule(0), decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#meaning all the result\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract mean to make it more normalize\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_test -= x_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "1563/1562 [==============================] - 13s - loss: 1.6407 - acc: 0.4378 - val_loss: 1.1300 - val_acc: 0.5942\n",
      "Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "1563/1562 [==============================] - 12s - loss: 1.2360 - acc: 0.5743 - val_loss: 0.9405 - val_acc: 0.6700\n",
      "Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "1563/1562 [==============================] - 12s - loss: 1.1161 - acc: 0.6212 - val_loss: 0.8716 - val_acc: 0.6983\n",
      "Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "1563/1562 [==============================] - 12s - loss: 1.0346 - acc: 0.6459 - val_loss: 0.9225 - val_acc: 0.7078\n",
      "Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.9640 - acc: 0.6677 - val_loss: 0.7102 - val_acc: 0.7535\n",
      "Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.8995 - acc: 0.6927 - val_loss: 0.7489 - val_acc: 0.7534\n",
      "Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.8466 - acc: 0.7106 - val_loss: 0.7067 - val_acc: 0.7539\n",
      "Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.8034 - acc: 0.7231 - val_loss: 0.6985 - val_acc: 0.7677\n",
      "Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.7659 - acc: 0.7345 - val_loss: 0.6555 - val_acc: 0.7769\n",
      "Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.7414 - acc: 0.7456 - val_loss: 0.6159 - val_acc: 0.7948\n",
      "Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.7256 - acc: 0.7529 - val_loss: 0.5818 - val_acc: 0.8038\n",
      "Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.7102 - acc: 0.7562 - val_loss: 0.5621 - val_acc: 0.8107\n",
      "Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6996 - acc: 0.7636 - val_loss: 0.5702 - val_acc: 0.8068\n",
      "Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6866 - acc: 0.7654 - val_loss: 0.5861 - val_acc: 0.8047\n",
      "Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6871 - acc: 0.7671 - val_loss: 0.5707 - val_acc: 0.8130\n",
      "Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6733 - acc: 0.7718 - val_loss: 0.6684 - val_acc: 0.7786\n",
      "Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6705 - acc: 0.7750 - val_loss: 0.6391 - val_acc: 0.7864\n",
      "Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6623 - acc: 0.7762 - val_loss: 0.5475 - val_acc: 0.8192\n",
      "Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6546 - acc: 0.7816 - val_loss: 0.5747 - val_acc: 0.8238\n",
      "Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6512 - acc: 0.7822 - val_loss: 0.5787 - val_acc: 0.8091\n",
      "Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6442 - acc: 0.7853 - val_loss: 0.5590 - val_acc: 0.8125\n",
      "Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6406 - acc: 0.7866 - val_loss: 0.4961 - val_acc: 0.8345\n",
      "Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6361 - acc: 0.7866 - val_loss: 0.6443 - val_acc: 0.7971\n",
      "Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6295 - acc: 0.7874 - val_loss: 0.4922 - val_acc: 0.8355\n",
      "Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6257 - acc: 0.7935 - val_loss: 0.6007 - val_acc: 0.8167\n",
      "Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6231 - acc: 0.7929 - val_loss: 0.5909 - val_acc: 0.8118\n",
      "Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6206 - acc: 0.7940 - val_loss: 0.5471 - val_acc: 0.8320\n",
      "Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6173 - acc: 0.7964 - val_loss: 0.5346 - val_acc: 0.8302\n",
      "Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6124 - acc: 0.7980 - val_loss: 0.4979 - val_acc: 0.8357\n",
      "Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6100 - acc: 0.7987 - val_loss: 0.5479 - val_acc: 0.8210\n",
      "Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6053 - acc: 0.7997 - val_loss: 0.4788 - val_acc: 0.8441\n",
      "Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6053 - acc: 0.8016 - val_loss: 0.5523 - val_acc: 0.8216\n",
      "Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6088 - acc: 0.7996 - val_loss: 0.5206 - val_acc: 0.8285\n",
      "Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6093 - acc: 0.7991 - val_loss: 0.5113 - val_acc: 0.8371\n",
      "Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6008 - acc: 0.8020 - val_loss: 0.5467 - val_acc: 0.8312\n",
      "Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6004 - acc: 0.8027 - val_loss: 0.5654 - val_acc: 0.8348\n",
      "Learning rate:  0.001\n",
      "Epoch 37/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6006 - acc: 0.8054 - val_loss: 0.5016 - val_acc: 0.8431\n",
      "Learning rate:  0.001\n",
      "Epoch 38/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6091 - acc: 0.8014 - val_loss: 0.6203 - val_acc: 0.8034\n",
      "Learning rate:  0.001\n",
      "Epoch 39/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5953 - acc: 0.8041 - val_loss: 0.6261 - val_acc: 0.8037\n",
      "Learning rate:  0.001\n",
      "Epoch 40/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5989 - acc: 0.8031 - val_loss: 0.6187 - val_acc: 0.8198\n",
      "Learning rate:  0.001\n",
      "Epoch 41/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5955 - acc: 0.8081 - val_loss: 0.7644 - val_acc: 0.7738\n",
      "Learning rate:  0.001\n",
      "Epoch 42/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6014 - acc: 0.8040 - val_loss: 0.5172 - val_acc: 0.8409\n",
      "Learning rate:  0.001\n",
      "Epoch 43/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5987 - acc: 0.8055 - val_loss: 0.4711 - val_acc: 0.8483\n",
      "Learning rate:  0.001\n",
      "Epoch 44/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5993 - acc: 0.8063 - val_loss: 0.4829 - val_acc: 0.8423\n",
      "Learning rate:  0.001\n",
      "Epoch 45/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5900 - acc: 0.8082 - val_loss: 0.5769 - val_acc: 0.8031\n",
      "Learning rate:  0.001\n",
      "Epoch 46/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5993 - acc: 0.8060 - val_loss: 0.4896 - val_acc: 0.8430\n",
      "Learning rate:  0.001\n",
      "Epoch 47/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6074 - acc: 0.8030 - val_loss: 0.4896 - val_acc: 0.8353\n",
      "Learning rate:  0.001\n",
      "Epoch 48/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6029 - acc: 0.8026 - val_loss: 0.6304 - val_acc: 0.8251\n",
      "Learning rate:  0.001\n",
      "Epoch 49/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.5984 - acc: 0.8072 - val_loss: 0.5364 - val_acc: 0.8224\n",
      "Learning rate:  0.001\n",
      "Epoch 50/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6053 - acc: 0.8029 - val_loss: 0.4591 - val_acc: 0.8533\n",
      "Learning rate:  0.001\n",
      "Epoch 51/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6049 - acc: 0.8051 - val_loss: 0.6122 - val_acc: 0.8105\n",
      "Learning rate:  0.001\n",
      "Epoch 52/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6027 - acc: 0.8041 - val_loss: 0.5054 - val_acc: 0.8313\n",
      "Learning rate:  0.001\n",
      "Epoch 53/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6033 - acc: 0.8054 - val_loss: 0.6801 - val_acc: 0.7985\n",
      "Learning rate:  0.001\n",
      "Epoch 54/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6051 - acc: 0.8049 - val_loss: 0.4976 - val_acc: 0.8376\n",
      "Learning rate:  0.001\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 12s - loss: 0.6048 - acc: 0.8038 - val_loss: 0.4870 - val_acc: 0.8373\n",
      "Learning rate:  0.001\n",
      "Epoch 56/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6098 - acc: 0.8048 - val_loss: 0.4931 - val_acc: 0.8403\n",
      "Learning rate:  0.001\n",
      "Epoch 57/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6018 - acc: 0.8046 - val_loss: 0.8323 - val_acc: 0.7621\n",
      "Learning rate:  0.001\n",
      "Epoch 58/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6082 - acc: 0.8040 - val_loss: 0.5988 - val_acc: 0.8287\n",
      "Learning rate:  0.001\n",
      "Epoch 59/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6068 - acc: 0.8042 - val_loss: 1.0337 - val_acc: 0.7629\n",
      "Learning rate:  0.001\n",
      "Epoch 60/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6153 - acc: 0.8020 - val_loss: 0.4593 - val_acc: 0.8518\n",
      "Learning rate:  0.001\n",
      "Epoch 61/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6086 - acc: 0.8044 - val_loss: 0.5422 - val_acc: 0.8358\n",
      "Learning rate:  0.001\n",
      "Epoch 62/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6127 - acc: 0.8027 - val_loss: 0.5351 - val_acc: 0.8358\n",
      "Learning rate:  0.001\n",
      "Epoch 63/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6112 - acc: 0.8044 - val_loss: 0.5529 - val_acc: 0.8297\n",
      "Learning rate:  0.001\n",
      "Epoch 64/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6112 - acc: 0.8040 - val_loss: 0.5487 - val_acc: 0.8392\n",
      "Learning rate:  0.001\n",
      "Epoch 65/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6102 - acc: 0.8037 - val_loss: 0.4775 - val_acc: 0.8462\n",
      "Learning rate:  0.001\n",
      "Epoch 66/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6118 - acc: 0.8008 - val_loss: 0.4670 - val_acc: 0.8472\n",
      "Learning rate:  0.001\n",
      "Epoch 67/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6101 - acc: 0.8048 - val_loss: 0.5492 - val_acc: 0.8382\n",
      "Learning rate:  0.001\n",
      "Epoch 68/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6137 - acc: 0.8042 - val_loss: 0.4581 - val_acc: 0.8505\n",
      "Learning rate:  0.001\n",
      "Epoch 69/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6091 - acc: 0.8066 - val_loss: 0.4923 - val_acc: 0.8430\n",
      "Learning rate:  0.001\n",
      "Epoch 70/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6081 - acc: 0.8050 - val_loss: 0.5620 - val_acc: 0.8177\n",
      "Learning rate:  0.001\n",
      "Epoch 71/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6121 - acc: 0.8047 - val_loss: 0.4695 - val_acc: 0.8502\n",
      "Learning rate:  0.001\n",
      "Epoch 72/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6133 - acc: 0.8038 - val_loss: 0.4824 - val_acc: 0.8476\n",
      "Learning rate:  0.001\n",
      "Epoch 73/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6152 - acc: 0.8038 - val_loss: 0.4968 - val_acc: 0.8428\n",
      "Learning rate:  0.001\n",
      "Epoch 74/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6155 - acc: 0.8045 - val_loss: 0.6130 - val_acc: 0.8262\n",
      "Learning rate:  0.001\n",
      "Epoch 75/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6112 - acc: 0.8034 - val_loss: 0.6152 - val_acc: 0.8241\n",
      "Learning rate:  0.001\n",
      "Epoch 76/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6122 - acc: 0.8029 - val_loss: 0.4893 - val_acc: 0.8403\n",
      "Learning rate:  0.001\n",
      "Epoch 77/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6124 - acc: 0.8031 - val_loss: 0.5013 - val_acc: 0.8418\n",
      "Learning rate:  0.001\n",
      "Epoch 78/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6219 - acc: 0.8020 - val_loss: 0.4773 - val_acc: 0.8460\n",
      "Learning rate:  0.001\n",
      "Epoch 79/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6058 - acc: 0.8058 - val_loss: 0.4693 - val_acc: 0.8469\n",
      "Learning rate:  0.001\n",
      "Epoch 80/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6121 - acc: 0.8048 - val_loss: 0.6374 - val_acc: 0.8217\n",
      "Learning rate:  0.001\n",
      "Epoch 81/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.6173 - acc: 0.8038 - val_loss: 0.4850 - val_acc: 0.8441\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4715 - acc: 0.8443 - val_loss: 0.4167 - val_acc: 0.8674\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4544 - acc: 0.8498 - val_loss: 0.4242 - val_acc: 0.8659\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4462 - acc: 0.8512 - val_loss: 0.4123 - val_acc: 0.8689\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4426 - acc: 0.8540 - val_loss: 0.4084 - val_acc: 0.8695\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4413 - acc: 0.8521 - val_loss: 0.4038 - val_acc: 0.8692\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4410 - acc: 0.8530 - val_loss: 0.4029 - val_acc: 0.8707\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4394 - acc: 0.8529 - val_loss: 0.4152 - val_acc: 0.8674\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4386 - acc: 0.8548 - val_loss: 0.4020 - val_acc: 0.8683\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4370 - acc: 0.8544 - val_loss: 0.4025 - val_acc: 0.8680\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4245 - acc: 0.8573 - val_loss: 0.3975 - val_acc: 0.8726\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4348 - acc: 0.8541 - val_loss: 0.4079 - val_acc: 0.8688\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4366 - acc: 0.8551 - val_loss: 0.4022 - val_acc: 0.8703\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4278 - acc: 0.8575 - val_loss: 0.4028 - val_acc: 0.8677\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4308 - acc: 0.8552 - val_loss: 0.4039 - val_acc: 0.8704\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4349 - acc: 0.8559 - val_loss: 0.4039 - val_acc: 0.8675\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4263 - acc: 0.8578 - val_loss: 0.3931 - val_acc: 0.8717\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4306 - acc: 0.8569 - val_loss: 0.3962 - val_acc: 0.8739\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4245 - acc: 0.8577 - val_loss: 0.3918 - val_acc: 0.8733\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/100\n",
      "1563/1562 [==============================] - 12s - loss: 0.4282 - acc: 0.8584 - val_loss: 0.3972 - val_acc: 0.8737\n",
      "Saved trained model at /home/thanawit/Softcom Final Project/saved_models/keras_cifar10_trained_model.h5 \n"
     ]
    }
   ],
   "source": [
    "#Decaying LR\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [ lr_reducer, lr_scheduler]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False,\n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,\n",
    "    # divide inputs by std of dataset\n",
    "    featurewise_std_normalization=False,\n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,\n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0.,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,\n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,\n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,\n",
    "    # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None)\n",
    "    \n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#train model \n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        callbacks=callbacks, steps_per_epoch = 50000/batch_size)\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
